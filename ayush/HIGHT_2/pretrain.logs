nohup: ignoring input
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
PyTorch: setting up devices
[W CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
loading configuration file config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/config.json
You are using a model of type llama to instantiate a model of type llava_graph. This is not supported for all configurations of models and can yield errors.
Model config LlavaGraphLlamaConfig {
  "_name_or_path": "/home/ubuntu/model_weights/vicuna-7b-v1.3",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llava_graph",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 32000
}

loading configuration file config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/config.json
You are using a model of type llama to instantiate a model of type llava_graph. This is not supported for all configurations of models and can yield errors.
Model config LlavaGraphLlamaConfig {
  "_name_or_path": "/home/ubuntu/model_weights/vicuna-7b-v1.3",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llava_graph",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 32000
}

loading weights file pytorch_model.bin from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/pytorch_model.bin.index.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.24s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.24s/it]
All model checkpoint weights were used when initializing LlavaGraphLlamaForCausalLM.

All the weights of LlavaGraphLlamaForCausalLM were initialized from the model checkpoint at lmsys/vicuna-7b-v1.3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaGraphLlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.25s/it]
All model checkpoint weights were used when initializing LlavaGraphLlamaForCausalLM.

All the weights of LlavaGraphLlamaForCausalLM were initialized from the model checkpoint at lmsys/vicuna-7b-v1.3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaGraphLlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

loading configuration file generation_config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

loading file tokenizer.model from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/tokenizer.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/special_tokens_map.json
loading file tokenizer_config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/tokenizer_config.json
loading file tokenizer.json from cache at None
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
loading file tokenizer.model from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/tokenizer.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/special_tokens_map.json
loading file tokenizer_config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/tokenizer_config.json
loading file tokenizer.json from cache at None
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Using auto half precision backend
***** Running training *****
  Num examples = 50,000
  Num Epochs = 2
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 2
  Total optimization steps = 1,562
  Number of trainable parameters = 3,796,992
  0%|          | 0/1562 [00:00<?, ?it/s]/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1673: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1673: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/1562 [00:11<4:54:09, 11.31s/it]  0%|          | 2/1562 [00:17<3:36:27,  8.33s/it]  0%|          | 3/1562 [00:22<2:57:13,  6.82s/it]  0%|          | 4/1562 [00:27<2:42:23,  6.25s/it]  0%|          | 5/1562 [00:33<2:38:00,  6.09s/it]  0%|          | 6/1562 [00:40<2:41:16,  6.22s/it]  0%|          | 7/1562 [00:46<2:39:49,  6.17s/it]  1%|          | 8/1562 [00:52<2:43:42,  6.32s/it]  1%|          | 9/1562 [00:58<2:36:48,  6.06s/it]  1%|          | 10/1562 [01:08<3:06:33,  7.21s/it]  1%|          | 11/1562 [01:14<2:57:21,  6.86s/it]  1%|          | 12/1562 [01:20<2:51:31,  6.64s/it]  1%|          | 13/1562 [01:25<2:43:01,  6.31s/it][22:42:13] WARNING: not removing hydrogen atom without neighbors
[22:42:13] WARNING: not removing hydrogen atom without neighbors
[22:42:13] WARNING: not removing hydrogen atom without neighbors
[22:42:13] WARNING: not removing hydrogen atom without neighbors
  1%|          | 14/1562 [01:33<2:49:11,  6.56s/it]  1%|          | 15/1562 [01:41<3:00:07,  6.99s/it]  1%|          | 16/1562 [01:46<2:46:54,  6.48s/it]  1%|          | 17/1562 [01:52<2:44:59,  6.41s/it]  1%|          | 18/1562 [01:59<2:49:13,  6.58s/it]  1%|          | 19/1562 [02:06<2:52:22,  6.70s/it]  1%|▏         | 20/1562 [02:13<2:56:35,  6.87s/it][22:43:04] WARNING: not removing hydrogen atom without neighbors
[22:43:04] WARNING: not removing hydrogen atom without neighbors
[22:43:04] WARNING: not removing hydrogen atom without neighbors
[22:43:04] WARNING: not removing hydrogen atom without neighbors
[22:43:04] WARNING: not removing hydrogen atom without neighbors
[22:43:04] WARNING: not removing hydrogen atom without neighbors
  1%|▏         | 21/1562 [02:19<2:49:20,  6.59s/it]  1%|▏         | 22/1562 [02:24<2:37:37,  6.14s/it]  1%|▏         | 23/1562 [02:32<2:50:08,  6.63s/it]  2%|▏         | 24/1562 [02:39<2:53:56,  6.79s/it]  2%|▏         | 25/1562 [02:44<2:38:08,  6.17s/it]  2%|▏         | 26/1562 [02:52<2:49:53,  6.64s/it]  2%|▏         | 27/1562 [02:58<2:47:03,  6.53s/it]  2%|▏         | 28/1562 [03:05<2:50:27,  6.67s/it]  2%|▏         | 29/1562 [03:12<2:50:07,  6.66s/it]  2%|▏         | 30/1562 [03:18<2:46:52,  6.54s/it]  2%|▏         | 31/1562 [03:26<3:00:57,  7.09s/it]  2%|▏         | 32/1562 [03:32<2:52:20,  6.76s/it]  2%|▏         | 33/1562 [03:39<2:49:51,  6.67s/it]  2%|▏         | 34/1562 [03:45<2:49:52,  6.67s/it]  2%|▏         | 35/1562 [03:50<2:30:27,  5.91s/it]  2%|▏         | 36/1562 [03:56<2:34:45,  6.09s/it]  2%|▏         | 37/1562 [04:01<2:24:22,  5.68s/it]  2%|▏         | 38/1562 [04:06<2:21:21,  5.57s/it]  2%|▏         | 39/1562 [04:12<2:27:32,  5.81s/it]  3%|▎         | 40/1562 [04:18<2:24:25,  5.69s/it]  3%|▎         | 41/1562 [04:23<2:19:01,  5.48s/it]  3%|▎         | 42/1562 [04:29<2:26:26,  5.78s/it]  3%|▎         | 43/1562 [04:36<2:33:00,  6.04s/it]  3%|▎         | 44/1562 [04:42<2:30:32,  5.95s/it]