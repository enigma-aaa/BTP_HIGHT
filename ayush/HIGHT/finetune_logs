/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
PyTorch: setting up devices
WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.
loading configuration file config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/config.json
You are using a model of type llama to instantiate a model of type llava_graph. This is not supported for all configurations of models and can yield errors.
Model config LlavaGraphLlamaConfig {
  "_name_or_path": "/home/ubuntu/model_weights/vicuna-7b-v1.3",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llava_graph",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 32000
}

loading configuration file config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/config.json
You are using a model of type llama to instantiate a model of type llava_graph. This is not supported for all configurations of models and can yield errors.
Model config LlavaGraphLlamaConfig {
  "_name_or_path": "/home/ubuntu/model_weights/vicuna-7b-v1.3",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llava_graph",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 32000
}

loading weights file pytorch_model.bin from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/pytorch_model.bin.index.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]loading weights file pytorch_model.bin from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/pytorch_model.bin.index.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.52s/it]
All model checkpoint weights were used when initializing LlavaGraphLlamaForCausalLM.

All the weights of LlavaGraphLlamaForCausalLM were initialized from the model checkpoint at lmsys/vicuna-7b-v1.3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaGraphLlamaForCausalLM for predictions without further training.
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.37s/it]loading configuration file generation_config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.41s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.56s/it]
All model checkpoint weights were used when initializing LlavaGraphLlamaForCausalLM.

All the weights of LlavaGraphLlamaForCausalLM were initialized from the model checkpoint at lmsys/vicuna-7b-v1.3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaGraphLlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0
}

loading file tokenizer.model from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/tokenizer.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/special_tokens_map.json
loading file tokenizer_config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/tokenizer_config.json
loading file tokenizer.json from cache at None
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead
  warnings.warn(out)
loading file tokenizer.model from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/tokenizer.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/special_tokens_map.json
loading file tokenizer_config.json from cache at /mnt/hdd/gtoken/.cache/huggingface/hub/models--lmsys--vicuna-7b-v1.3/snapshots/236eeeab96f0dc2e463f2bebb7bb49809279c6d6/tokenizer_config.json
loading file tokenizer.json from cache at None
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/mnt/data1/gtoken/miniconda3/envs/env_hight_ayush/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead
  warnings.warn(out)
Using auto half precision backend
Attempting to resume from ./checkpoints/Graph-LLaVA-4C-hvqvae2-5ep-hlinear-fgprompt-neg-extend/graph-text-molgen/property_pred-llava-hvqvae2-lmsys/vicuna-7b-v1.3-finetune_lora-5ep/checkpoint-235
***** Running training *****
  Num examples = 1,500
  Num Epochs = 5
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 235
  Number of trainable parameters = 165,567,233
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 5
  Continuing training from global step 235
  Will skip the first 5 epochs then the first 0 batches in the first epoch.
  0%|          | 0/235 [00:00<?, ?it/s]

Training completed. Do not forget to share your model on huggingface.co/models =)


                                         0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 0/235 [00:00<?, ?it/s]
Configuration saved in ./checkpoints/Graph-LLaVA-4C-hvqvae2-5ep-hlinear-fgprompt-neg-extend/graph-text-molgen/property_pred-llava-hvqvae2-lmsys/vicuna-7b-v1.3-finetune_lora-5ep/config.json
